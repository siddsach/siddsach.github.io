---
title: 'The Real AI Control Problem'
date: 2018-01-16
permalink: /posts/2018/01/bias/
---

Conversations about AI safety are all the rage these days, but the kinds of things people are afraid of are often ill informed by what is actually happening in A.I. We aren’t even close to general-purpose agents that can adapt to new environments or plan diverse sets of actions to realize abstract goals, and it’s not clear how long progress to that end will take. What’s much clearer is that for a wide range of problems, if you give a model massive amounts of training data
 and a clear objective to optimize, it can do surprisingly well. Face recognition or sentiment analysis used to be unthinkably difficult for computers, now the technology is becoming standard. But just because the people's conception of an AI apocalypse is more informed by sci-fi than by evidence, that doesn't mean we shouldn't be worried about AI safety.

When the training data is biased, what happens to these marvelous algorithms? Google recently caught some flak because its sentiment analyzer classified sentences with the words “jew” and “gay” as negative. The case was significantly more concerning when a Propublica report found that a machine-learning tool used by courts to predict a felon’s likelihood of committing another crime systematically overestimated discriminated against African-Americans. The ramifications of this problem aren’t theoretical or set to happen in the far future; it’s happening right, as loan
approvals and news consumption become more and more determined by machine learning algorithms. 

## Building a Subjectivity Classifier

I encountered this problem myself when I built a system for Flipside that would classify a sentence as an opinionated or factual claim. The goal of this algorithm wasn't to determine the veracity of a particular claim or be a "fake news detector"; it just aimed to determine whether the claim is asserting something as objectively true or expressing a subjective belief. Modern machine learning is effective enough that I was able to rope together some out-of-the-box methods and and reach human-level accuracy of 80% on a news subjectivity dataset<sup>1</sup>. Of course, my dataset necessarily contained some bias, whether because the annotators come from a particular demographic of people or because the sentences only cover particular topics. But these problems are at least partially fixable by getting a more diverse set of annotators and making more training data; not an easy task, but at least clear cut. 

What's more interesting is that my algorithm's effectiveness was grounded in using word meanings trained on large datasets like Wikipedia and Google News. This is an example of a common practice in machine learning and NLP called transfer learning, where models are pretrained on large datasets to learn ground-truth low-level meaning to build on for high-level tasks. The problem is that the bias in ground-truth knowledge  is harder to correct, because you can't get unbiased data from a biased society. I wanted to see whether the differences in my choice of word representations could actually lead to interpretable bias. Inspired by work on self-attention mechanisms, a new kind of neural memory module that uses a weighting over the tokens in an input to aggregate evidence, I wanted to try visualizing these weights to interpretably look at what the model "sees", and by extension compare how different sources of ground-truth word meaning lead to different kinds of bias. It turns out, this works quite well. 

## Interpreting Attention

I used four settings: No word vectors (None), word vectors trained on Wikipedia (glove), word vectors trained on Google News (google), and word vectors I trained myself on Gigaword, an international news dataset (gigavec). 

![alt text](/images/17th_different_4112th_example.png)

This is a good illustrative example. Here, it’s clear that each model is considering differing pieces of evidence to make its decision. Even though they both classify the sentence as subjective, the gigavec model focuses on personal pronouns like “we” and “our” and the second model focuses on political words like “constitution” and “laws”, indicating different kinds of reasoning. The third model ignores the quotation mark and focuses on the generic pronoun "what", and thus
decides the sentece is subjective. In this way, we can see not only that the different models make different choices, but form reasonable hypotheses as to the reasoning behind those decisions and how that reasoning differs between the differing "perspectives" the word vectors offer. Let's consider a few other examples

![alt text](/images/164th_different_1324th_example.png)

In this case, the glove model demonstrates an ability to understand phrases in addition to words, as it’s able to recognize that the phrase “in line” is relevant to the sentence’s subjectivity, even though each word by itself is irrelevant to subjectivity. glove (Wikipedia) uses the biggest dataset, so this could be evidence of nuanced understanding its model has that the others don't. One also might notice that the model with word vectors (None) highlights most of the sentence. This
is likely because the model is memorizing parts of each example rather than actually understanding the sentence (that's why we use transfer learning!).

![alt text](/images/39th_different_1619th_example.png)

Just the phrase global warming issues caused the google news to classify the sentence as subjective, whereas the Wikipedia and International news dataset did the opposite. Just the volume of opinionated content in Google News that exists about global warming, causes the model to also learn that global warming is a subjective term.


![alt text](/images/163th_different_5824th_example.png)

Again, the google news model is showing some bias here. The words "iran" and "drug" were enough to trigger the model to classify it as subjective. These sorts of mistakes suggest that the model is using simple shortcuts to get likely correct rather than truly understanding the sentence. 


![alt text](/images/144th_different_2423th_example.png)

This example gives further credence to this idea. Simply because the sentence is about a scientific topic, both the Wikipedia and google news models fail to recognize the sentence as subjective. 

## So Biased Models are Biased. Now What?

These examples clearly demonstrate that data sources that are fairly representative of a wide range of domains and viewpoints can lead to socially harmful bias. However, some consistent patterns in these examples give hint to potential solutions.

* It seems many error tend to occur because the model is doing simple pattern matching rather than operating from a global understanding of the sentence. Perhaps this can be remedied by using transfer learning not just to give a model meanings of words, but also to endow it with more
global understanding of sentences of paragraphs (I'm working on using autoencoders and language models!)
* The kinds of reasoning the model uses is varied, but certainly falls into some set of buckets. In some cases, the topic defines the decisions such as the mention Iran or Global Warming, but in others, the style and word choice is the key component such as usage of personal pronouns or strong adjectives.  If we can make our model discretize these sets of reasoning, (I'm working on learning clusters in some vector space or mutually independent features learned through variational methods), it'll be easier to understand what is going on!

The problem of bias in machine learning is clearly a deep one. But I believe it can be fixed if we are less single-minded in our focus on accuracy rates and competitions and more deeply consider the social nuances of applications of these methods. With a broader perspective, we can make imbibe our A.I. not only with the breadth of human knowledge, but also with the nuance of our humanity.  



<sub><sup><sup>1</sup>Any conversation about applied machine learning has to start talk about the training data. I used an old academic dataset of sentences labeled by subjectivity because it contained news articles, the same domain my algorithm would have to perform in. Though these annotations highlighted subjective words and phrases rather than sentences, I was able to create pretty good sentence level annotations (80% human agreement) by checking whether a sentence had an intense subjective phrase
within it or not. I also cut out borderline cases with only mildly subjective phrases because if the human annotators are unsure about whether a sentence is subjective, the model definitely won’t understand (you can read more about this dataset here). But even with this data moxy, 10,000 labeled sentences just wasn’t enough training data for the model to perform at human-level. When I built an RNN Classifier on this data, it got a dismal 72% accuracy. Luckily, a common trick
of using pre-trained word representations helped me improve this to 81% accuracy.</sup></sub>

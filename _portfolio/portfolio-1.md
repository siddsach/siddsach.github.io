#(In Progress) 

What are we Transferring Anyway?: Using Attention Weights to Interpret domain choice in Transfer Learning

[Code](https://github.com/siddsach/Interpreting-Attention)

## Data

### Language Model Datasets
* Wikitext-2
* Gigaword
* Penn Tree Bank

### Text Classification Datasets
* IMDB Sentiment Classification
* MPQA Subjectivity Classification

### Word Vectors
* CharNGram
* Google News Word2Vec
* GloVe

###  Implemented
Data Collection and Cleaning
Modified Word Language Model
LSTM Classifier
Self-Attention Embedding
Key-Value Attention
Language Model Pretraining

### To Do
Classification Tuning
Attention Classification Tuning
Model Comparison


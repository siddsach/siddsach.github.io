---
title: "What are we Transferring Anyway?"
excerpt: "Using Attention Weights to Interpret domain choice in Transfer Learning<br/><img src='/images/rnn.png'>"
collection: portfolio
---

#(In Progress) 

[Code](https://github.com/siddsach/Interpreting-Attention)

A Research Project Experimenting with Self-Attention to interpret different source domains for Transfer Learning 

###  Implemented
* Data Collection and Cleaning
* Modified Word Language Model
* LSTM Classifier
* Self-Attention Embedding
* Key-Value Attention
* Language Model Pretraining

### To Do
* Classification Tuning
* Attention Classification Tuning
* Model Comparison

## Data

### Language Model Datasets
* Wikitext-2
* Gigaword
* Penn Tree Bank

### Text Classification Datasets
* IMDB Sentiment Classification
* MPQA Subjectivity Classification

### Word Vectors
* CharNGram
* Google News Word2Vec
* GloVe


